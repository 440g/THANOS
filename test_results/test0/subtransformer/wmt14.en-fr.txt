CUDA_VISIBLE_DEVICES=0 python ./train.py \
>   --configs=configs/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms.yml \
>   --sub-configs=configs/wmt14.en-fr/subtransformer/common64.yml \
>   --num-workers 8 \
>   | tee test_results/test0/subtransformer/wmt14.en-fr.txt

| Configs: Namespace(configs='configs/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms.yml', pdb=False, no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='checkpoints/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms/tensorboard', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='label_smoothed_cross_entropy', optimizer='adam', lr_scheduler='cosine', task='translation', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, max_tokens_valid=4096, max_sentences_valid=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, arch='transformersuper_wmt_en_fr', max_epoch=0, max_update=40000, clip_norm=0.0, sentence_avg=False, update_freq=[16], lr=[1e-07], min_lr=-1, use_bmuf=False, save_dir='checkpoints/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=20, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, train_subtransformer=True, sub_configs='configs/wmt14.en-fr/subtransformer/common64.yml', profile_flops=False, rknn_model=None, latnpu=False, latgpu=False, latcpu=False, latiter=300, latsilent=False, validate_subtransformer=False, path=None, remove_bpe=None, quiet=False, model_overrides='{}', results_path=None, beam=5, nbest=1, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, no_early_stop=False, unnormalized=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, print_alignment=False, profile_latency=False, no_token_positional_embeddings=False, get_attn=False, encoder_embed_choice=[512, 256, 128], decoder_embed_choice=[512, 256, 128], encoder_layer_num_choice=[7, 6, 5, 4, 3, 2], decoder_layer_num_choice=[7, 6, 5, 4, 3, 2], encoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], decoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], encoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_ende_attention_heads_choice=[16, 8, 4, 2, 1], qkv_dim=64, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], vocab_original_scaling=False, encoder_embed_dim_subtransformer=640, decoder_embed_dim_subtransformer=640, encoder_ffn_embed_dim_all_subtransformer=[1024, 2048, 1024, 2048, 3072, 2048], decoder_ffn_embed_dim_all_subtransformer=[1024, 2048, 3072, 2048, 2048, 2048], encoder_self_attention_heads_all_subtransformer=[8, 4, 8, 4, 4, 8], decoder_self_attention_heads_all_subtransformer=[4, 8, 4, 8, 8, 8], decoder_ende_attention_heads_all_subtransformer=[4, 4, 8, 4, 4, 8], decoder_arbitrary_ende_attn_all_subtransformer=[1, 2, -1, 2, 2, 1], label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, warmup_updates=4000, warmup_init_lr=1e-07, max_lr=0.001, t_mult=1, lr_period_updates=-1, lr_shrink=1.0, data='data/binary/wmt14_en_fr', source_lang=None, target_lang=None, lazy_load=False, raw_text=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, share_all_embeddings=True, dropout=0.3, attention_dropout=0.1, encoder_embed_dim=640, decoder_embed_dim=640, encoder_ffn_embed_dim=3072, decoder_ffn_embed_dim=3072, encoder_layers=6, decoder_layers=6, encoder_attention_heads=8, decoder_attention_heads=8, encoder_layer_num_subtransformer=6, decoder_layer_num_subtransformer=5, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=640, decoder_input_dim=640)
| [en] dictionary: 44512 types
| [fr] dictionary: 44512 types
| loaded 26854 examples from: data/binary/wmt14_en_fr/valid.en-fr.en
| loaded 26854 examples from: data/binary/wmt14_en_fr/valid.en-fr.fr
| data/binary/wmt14_en_fr valid en-fr 26854 examples
| Fallback to xavier initializer
| Model: transformersuper_wmt_en_fr 
| Criterion: LabelSmoothedCrossEntropyCriterion
 

		WARNING!!! Training one single SubTransformer


| SubTransformer Arch: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [1024, 2048, 1024, 2048, 3072, 2048], 'encoder_self_attention_heads': [8, 4, 8, 4, 4, 8]}, 'decoder': {'decoder_embed_dim': 640, 'decoder_layer_num': 5, 'decoder_ffn_embed_dim': [1024, 2048, 3072, 2048, 2048, 2048], 'decoder_self_attention_heads': [4, 8, 4, 8, 8, 8], 'decoder_ende_attention_heads': [4, 4, 8, 4, 4, 8], 'decoder_arbitrary_ende_attn': [1, 2, -1, 2, 2, 1]}} 

| SubTransformer size (without embedding weights): 30222976
| Embedding layer size: 28487680 

| Training on 1 GPUs
| Max tokens per GPU = 4096 and max sentences per GPU = None 

| no existing checkpoint found checkpoints/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms/checkpoint_last.pt
| loading train data for epoch 0
| loaded 35762584 examples from: data/binary/wmt14_en_fr/train.en-fr.en
| loaded 35762584 examples from: data/binary/wmt14_en_fr/train.en-fr.fr
| data/binary/wmt14_en_fr train en-fr 35762584 examples
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001 | loss 5.550 | nll_loss 4.023 | ppl 16.26 | wps 36082 | ups 1 | wpb 61071.998 | bsz 1644.983 | num_updates 21719 | lr 0.000512308 | gnorm 0.641 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 36762 | train_wall 35475
| epoch 001 | validate on 'valid' subset | loss 4.683 | nll_loss 2.952 | ppl 7.74 | num_updates 21719 | subtransformer_loss 4.683 | subtransformer_nll_loss 2.952
| saved checkpoint checkpoints/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms/checkpoint1.pt (epoch 1 @ 21719 updates) (writing took 1.478163480758667 seconds)
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 2.0
| WARNING: overflow detected, setting loss scale to: 4.0
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 002 | loss 4.789 | nll_loss 3.170 | ppl 9.00 | wps 36304 | ups 1 | wpb 61077.866 | bsz 1645.117 | num_updates 40000 | lr 0.001 | gnorm 0.536 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 67531 | train_wall 65246
| epoch 002 | validate on 'valid' subset | loss 4.523 | nll_loss 2.779 | ppl 6.86 | num_updates 40000 | best_loss 4.52319 | subtransformer_loss 4.523 | subtransformer_nll_loss 2.779
| saved checkpoint checkpoints/wmt14.en-fr/subtransformer/wmt14enfr_npu_test0@200ms/checkpoint_best.pt (epoch 2 @ 40000 updates) (writing took 1.2098369598388672 seconds)
| Done training in 67420.6 seconds
