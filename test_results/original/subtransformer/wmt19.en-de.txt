CUDA_VISIBLE_DEVICES=0 python ./train.py \
--configs=configs/wmt19.en-de/subtransformer/wmt19ende_npu@200ms.yml \
--sub-configs=configs/wmt19.en-de/subtransformer/common.yml \
--num-workers=8

| Configs: Namespace(configs='configs/wmt19.en-de/subtransformer/wmt19ende_npu@200ms.yml', pdb=False, no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms/tensorboard', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='label_smoothed_cross_entropy', optimizer='adam', lr_scheduler='cosine', task='translation', num_workers=8, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=10, disable_validation=False, max_tokens_valid=4096, max_sentences_valid=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, arch='transformersuper_wmt_en_de', max_epoch=0, max_update=40000, clip_norm=0.0, sentence_avg=False, update_freq=[16], lr=[1e-07], min_lr=-1, use_bmuf=False, save_dir='checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=20, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, train_subtransformer=True, sub_configs='configs/wmt19.en-de/subtransformer/common512.yml', profile_flops=False, rknn_model=None, latnpu=False, latgpu=False, latcpu=False, latiter=300, latsilent=False, validate_subtransformer=False, path=None, remove_bpe=None, quiet=False, model_overrides='{}', results_path=None, beam=5, nbest=1, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, no_early_stop=False, unnormalized=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, print_alignment=False, profile_latency=False, no_token_positional_embeddings=False, get_attn=False, encoder_embed_choice=[512, 256, 128], decoder_embed_choice=[512, 256, 128], encoder_layer_num_choice=[7, 6, 5, 4, 3, 2], decoder_layer_num_choice=[7, 6, 5, 4, 3, 2], encoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], decoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], encoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_ende_attention_heads_choice=[16, 8, 4, 2, 1], qkv_dim=512, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], vocab_original_scaling=False, encoder_embed_dim_subtransformer=640, decoder_embed_dim_subtransformer=640, encoder_ffn_embed_dim_all_subtransformer=[1024, 3072, 2048, 1024, 2048, 3072], decoder_ffn_embed_dim_all_subtransformer=[1024, 2048, 2048, 1024, 2048, 1024], encoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4, 4, 4], decoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4, 8, 8], decoder_ende_attention_heads_all_subtransformer=[4, 4, 8, 4, 4, 4], decoder_arbitrary_ende_attn_all_subtransformer=[1, 2, -1, 2, 2, -1], label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, warmup_updates=4000, warmup_init_lr=1e-07, max_lr=0.001, t_mult=1, lr_period_updates=-1, lr_shrink=1.0, data='data/binary/wmt19_en_de', source_lang=None, target_lang=None, lazy_load=False, raw_text=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, share_all_embeddings=True, dropout=0.3, attention_dropout=0.1, encoder_embed_dim=640, decoder_embed_dim=640, encoder_ffn_embed_dim=3072, decoder_ffn_embed_dim=3072, encoder_layers=6, decoder_layers=6, encoder_attention_heads=8, decoder_attention_heads=8, encoder_layer_num_subtransformer=6, decoder_layer_num_subtransformer=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=640, decoder_input_dim=640)
| [en] dictionary: 49600 types
| [de] dictionary: 49600 types
| loaded 2942 examples from: data/binary/wmt19_en_de/valid.en-de.en
| loaded 2942 examples from: data/binary/wmt19_en_de/valid.en-de.de
| data/binary/wmt19_en_de valid en-de 2942 examples
| Fallback to xavier initializer
| Model: transformersuper_wmt_en_de
| Criterion: LabelSmoothedCrossEntropyCriterion


                WARNING!!! Training one single SubTransformer


| SubTransformer Arch: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [1024, 3072, 2048, 1024, 2048, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 4, 4]}, 'decoder': {'decoder_embed_dim': 640, 'decoder_layer_num': 6, 'decoder_ffn_embed_dim': [1024, 2048, 2048, 1024, 2048, 1024], 'decoder_self_attention_heads': [8, 8, 8, 4, 8, 8], 'decoder_ende_attention_heads': [4, 4, 8, 4, 4, 4], 'decoder_arbitrary_ende_attn': [1, 2, -1, 2, 2, -1]}}

| SubTransformer size (without embedding weights): 51224832
| Embedding layer size: 31744000

| Training on 1 GPUs
| Max tokens per GPU = 4096 and max sentences per GPU = None

| no existing checkpoint found checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms/checkpoint_last.pt
| loading train data for epoch 0
| loaded 43037018 examples from: data/binary/wmt19_en_de/train.en-de.en
| loaded 43037018 examples from: data/binary/wmt19_en_de/train.en-de.de
| data/binary/wmt19_en_de train en-de 43037018 examples
| epoch 001:   0%|                                                                  | 0/24837 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 64.0
| epoch 001:   0%|                                                        | 1/24837 [00:01<8:39:30,  1.26s/it]| WARNING: overflow detected, setting loss scale to: 32.0
| epoch 001:   0%|                                                       | 2/24837 [00:07<27:51:36,  4.04s/it]| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001:   0%|                                                       | 3/24837 [00:17<46:46:08,  6.78s/it]| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001:   0%|                                                       | 4/24837 [00:22<42:11:22,  6.12s/it]/home/mk/eiai/THANOS/fairseq/optim/adam.py:159: UserWarning: This overload of addcdiv_ is deprecated:
        addcdiv_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
        addcdiv_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)
  p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
| epoch 001:   8%| | 2067/24837 [2:07:35<19:30:12,  3.08s/it, loss=9.759, wps=15941, num_updates=2063, lr=0.00| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001:   9%| | 2206/24837 [2:16:22<25:18:08,  4.02s/it, loss=9.603, wps=15931, num_updates=2201, lr=0.00| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001:  13%|▏| 3245/24837 [3:21:01<21:19:36,  3.56s/it, loss=8.696, wps=15999, num_updates=3239, lr=0.00| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001:  17%|▏| 4339/24837 [4:27:43<16:30:37,  2.90s/it, loss=8.089, wps=16116, num_updates=4332, lr=0.00| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001:  19%|▏| 4688/24837 [4:49:20<25:23:43,  4.54s/it, loss=7.937, wps=16122, num_updates=4680, lr=0.00| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  23%|▏| 5831/24837 [5:59:25<22:20:10,  4.23s/it, loss=7.539, wps=16170, num_updates=5822, lr=0.00| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  28%|▎| 6887/24837 [7:04:47<16:11:02,  3.25s/it, loss=7.263, wps=16178, num_updates=6877, lr=0.00| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  32%|▎| 8045/24837 [8:15:35<18:36:54,  3.99s/it, loss=7.028, wps=16210, num_updates=8034, lr=0.00| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  37%|▎| 9075/24837 [9:19:31<16:31:36,  3.77s/it, loss=6.858, wps=16207, num_updates=9063, lr=0.00| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  41%|▍| 10109/24837 [10:24:25<13:42:26,  3.35s/it, loss=6.717, wps=16186, num_updates=10096, lr=0| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  45%|▍| 11180/24837 [11:29:32<13:13:16,  3.49s/it, loss=6.592, wps=16216, num_updates=11166, lr=0| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001:  47%|▍| 11740/24837 [12:04:14<12:46:05,  3.51s/it, loss=6.534, wps=16214, num_updates=11725, lr=0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  53%|▌| 13214/24837 [13:35:28<11:47:38,  3.65s/it, loss=6.400, wps=16215, num_updates=13198, lr=0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  58%|▌| 14396/24837 [14:48:21<9:20:00,  3.22s/it, loss=6.308, wps=16222, num_updates=14379, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  62%|▌| 15306/24837 [15:44:23<10:23:36,  3.93s/it, loss=6.245, wps=16226, num_updates=15288, lr=0| WARNING: overflow detected, setting loss scale to: 1.0
| epoch 001:  70%|▋| 17395/24837 [17:52:02<7:20:45,  3.55s/it, loss=6.119, wps=16251, num_updates=17376, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  74%|▋| 18484/24837 [18:59:58<7:49:25,  4.43s/it, loss=6.062, wps=16241, num_updates=18464, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  79%|▊| 19516/24837 [20:04:30<5:09:19,  3.49s/it, loss=6.011, wps=16231, num_updates=19495, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  83%|▊| 20573/24837 [21:10:37<4:10:08,  3.52s/it, loss=5.963, wps=16223, num_updates=20551, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  87%|▊| 21612/24837 [22:14:32<3:50:59,  4.30s/it, loss=5.920, wps=16226, num_updates=21589, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  91%|▉| 22641/24837 [23:17:32<1:48:08,  2.95s/it, loss=5.880, wps=16234, num_updates=22617, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001:  95%|▉| 23672/24837 [24:24:36<1:20:49,  4.16s/it, loss=5.842, wps=16197, num_updates=23647, lr=0.| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001: 100%|▉| 24751/24837 [25:31:22<04:48,  3.35s/it, loss=5.804, wps=16198, num_updates=24725, lr=0.00| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 001 | loss 5.802 | nll_loss 4.278 | ppl 19.40 | wps 16199 | ups 0 | wpb 60289.782 | bsz 1732.991 | num_updates 24810 | lr 0.000378678 | gnorm 0.607 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 92337 | train_wall 90895
| epoch 002:   4%| | 1028/24837 [1:03:31<24:58:46,  3.78s/it, loss=4.948, wps=16266, num_updates=25838, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:   8%| | 2092/24837 [2:09:16<21:48:13,  3.45s/it, loss=4.941, wps=16260, num_updates=26901, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  13%|▏| 3309/24837 [3:24:43<25:37:25,  4.28s/it, loss=4.929, wps=16236, num_updates=28117, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  18%|▏| 4348/24837 [4:28:22<22:31:46,  3.96s/it, loss=4.921, wps=16271, num_updates=29155, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  23%|▏| 5640/24837 [5:47:42<17:17:01,  3.24s/it, loss=4.911, wps=16291, num_updates=30446, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  29%|▎| 7164/24837 [7:21:29<14:55:50,  3.04s/it, loss=4.899, wps=16295, num_updates=31969, lr=0.0| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  33%|▎| 8316/24837 [8:32:15<15:20:38,  3.34s/it, loss=4.892, wps=16301, num_updates=33120, lr=8.7| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  43%|▍| 10558/24837 [10:52:00<14:27:34,  3.65s/it, loss=4.877, wps=16261, num_updates=35361, lr=4| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 002:  44%|▍| 10837/24837 [11:09:16<13:20:27,  3.43s/it, loss=4.876, wps=16260, num_updates=35639, lr=3| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  48%|▍| 11980/24837 [12:19:48<11:41:19,  3.27s/it, loss=4.870, wps=16260, num_updates=36781, lr=1| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002:  57%|▌| 14055/24837 [14:26:02<10:21:02,  3.46s/it, loss=4.861, wps=16295, num_updates=38855, lr=2| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 002:  61%|▌| 15132/24837 [15:33:14<9:51:38,  3.66s/it, loss=4.857, wps=16281, num_updates=39931, lr=1.| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 002 | loss 4.857 | nll_loss 3.220 | ppl 9.32 | wps 16278 | ups 0 | wpb 60285.668 | bsz 1730.646 | num_updates 40000 | lr 0.001 | gnorm 0.467 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 148595 | train_wall 146334
| Done training in 148445.3 seconds