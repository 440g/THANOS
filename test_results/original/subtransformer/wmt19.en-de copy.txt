python ./train.py --configs=configs/wmt19.en-de/subtransformer/wmt19ende_npu@200ms.yml \
    --sub-configs=configs/wmt19.en-de/subtransformer/common.yml
/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
| Configs: Namespace(configs='configs/wmt19.en-de/subtransformer/wmt19ende_npu@200ms.yml', pdb=False, no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms/tensorboard', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='label_smoothed_cross_entropy', optimizer='adam', lr_scheduler='cosine', task='translation', num_workers=10, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=10, disable_validation=False, max_tokens_valid=4096, max_sentences_valid=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, arch='transformersuper_wmt_en_de', max_epoch=0, max_update=40000, clip_norm=0.0, sentence_avg=False, update_freq=[16], lr=[1e-07], min_lr=-1, use_bmuf=False, save_dir='checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=10, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=20, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, train_subtransformer=True, sub_configs='configs/wmt19.en-de/subtransformer/common.yml', profile_flops=False, rknn_model=None, latnpu=False, latgpu=False, latcpu=False, latiter=300, latsilent=False, validate_subtransformer=False, path=None, remove_bpe=None, quiet=False, model_overrides='{}', results_path=None, beam=5, nbest=1, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, no_early_stop=False, unnormalized=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, print_alignment=False, profile_latency=False, no_token_positional_embeddings=False, get_attn=False, encoder_embed_choice=[512, 256, 128], decoder_embed_choice=[512, 256, 128], encoder_layer_num_choice=[7, 6, 5, 4, 3, 2], decoder_layer_num_choice=[7, 6, 5, 4, 3, 2], encoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], decoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], encoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_self_attention_heads_choice=[16, 8, 4, 2, 1], decoder_ende_attention_heads_choice=[16, 8, 4, 2, 1], qkv_dim=512, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], vocab_original_scaling=False, encoder_embed_dim_subtransformer=640, decoder_embed_dim_subtransformer=640, encoder_ffn_embed_dim_all_subtransformer=[1024, 3072, 2048, 1024, 2048, 3072], decoder_ffn_embed_dim_all_subtransformer=[1024, 2048, 2048, 1024, 2048, 1024], encoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4, 4, 4], decoder_self_attention_heads_all_subtransformer=[8, 8, 8, 4, 8, 8], decoder_ende_attention_heads_all_subtransformer=[4, 4, 8, 4, 4, 4], decoder_arbitrary_ende_attn_all_subtransformer=[1, 2, -1, 2, 2, -1], label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, warmup_updates=4000, warmup_init_lr=1e-07, max_lr=0.001, t_mult=1, lr_period_updates=-1, lr_shrink=1.0, data='data/binary/wmt19_en_de', source_lang=None, target_lang=None, lazy_load=False, raw_text=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, share_all_embeddings=True, dropout=0.3, attention_dropout=0.1, encoder_embed_dim=640, decoder_embed_dim=640, encoder_ffn_embed_dim=3072, decoder_ffn_embed_dim=3072, encoder_layers=6, decoder_layers=6, encoder_attention_heads=8, decoder_attention_heads=8, encoder_layer_num_subtransformer=6, decoder_layer_num_subtransformer=6, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=640, decoder_input_dim=640)
| [en] dictionary: 49600 types
| [de] dictionary: 49600 types
| loaded 2942 examples from: data/binary/wmt19_en_de/valid.en-de.en
| loaded 2942 examples from: data/binary/wmt19_en_de/valid.en-de.de
| data/binary/wmt19_en_de valid en-de 2942 examples
| Fallback to xavier initializer
| Model: transformersuper_wmt_en_de
| Criterion: LabelSmoothedCrossEntropyCriterion


                WARNING!!! Training one single SubTransformer


| SubTransformer Arch: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [1024, 3072, 2048, 1024, 2048, 3072], 'encoder_self_attention_heads': [8, 8, 8, 4, 4, 4]}, 'decoder': {'decoder_embed_dim': 640, 'decoder_layer_num': 6, 'decoder_ffn_embed_dim': [1024, 2048, 2048, 1024, 2048, 1024], 'decoder_self_attention_heads': [8, 8, 8, 4, 8, 8], 'decoder_ende_attention_heads': [4, 4, 8, 4, 4, 4], 'decoder_arbitrary_ende_attn': [1, 2, -1, 2, 2, -1]}}

| SubTransformer size (without embedding weights): 51224832
| Embedding layer size: 31744000

| Training on 1 GPUs
| Max tokens per GPU = 4096 and max sentences per GPU = None

| no existing checkpoint found checkpoints/wmt19.en-de/subtransformer/wmt19ende_npu@200ms/checkpoint_last.pt
| loading train data for epoch 0
| loaded 43037018 examples from: data/binary/wmt19_en_de/train.en-de.en
| loaded 43037018 examples from: data/binary/wmt19_en_de/train.en-de.de
| data/binary/wmt19_en_de train en-de 43037018 examples
| epoch 001:   0%|                          | 0/24837 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 64.0
| epoch 001:   0%|     | 1/24837 [10:24:02<258309:22:58, 37442.17s/it]Killed